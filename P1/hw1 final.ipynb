{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f54b267",
   "metadata": {},
   "source": [
    "# امیرمهدی کوششی ۹۸۱۷۱۰۵۳\n",
    "# محمد صادق مجیدی ۹۸۱۰۶۰۰۴\n",
    "# امیرحسین باقری ۹۸۱۰۵۶۲۱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d41f3b",
   "metadata": {},
   "source": [
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "در این تمرین قصد داریم که وسیله نقلیه را در متن پیدا کنیم اگر وسیله نقلیه در متن پیدا شد مبدا و مقصد را نیز پیدا می‌کنیم.\n",
    "            </font>\n",
    "</div>\n",
    "\n",
    "## الگوریتم حل\n",
    "\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=4>\n",
    "ابندا قصد داشتیم که مبدا ها و مقصد هارا نیز با استفاده از دیتاست شهر ها پیدا کنیم اما به این نکته برخوردیم که هزاران هزار ترکیب اسمی می‌توانند مبدا و مقصد باشند بنابراین تصمیم بر آن شد که \n",
    "از تگر هضم استفاده کنیم. و گروه های اسمی را به عنوان مبدا و مقصد اعلام کنیم.\n",
    "            </font>\n",
    "</div>\n",
    "\n",
    "### استفاده از هضم\n",
    "\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=4>\n",
    "با توجه به مثال های داده شده در داک تمرین مبدا و مقصد و وسیله نقلیه را در یک جمله پیدا می‌کنیم.\n",
    "برای جمله بندی رشته داده شده از ابزار هضم استفاده می‌کنیم.\n",
    "            </font>\n",
    "</div>\n",
    "\n",
    "\n",
    "```c\n",
    "    def get_analysis(self,token, mode = 0):\n",
    "        assert 0 <= mode < 5 ,\"0  <= mode < 5\"\n",
    "        if mode == 0:\n",
    "            return self.lemmatizer.lemmatize(token)\n",
    "        if mode == 1:\n",
    "            return self.stemmer.stem(token)\n",
    "        if mode == 2:\n",
    "            return self.normalizer.normalize(token)\n",
    "        if mode == 3:\n",
    "            return sent_tokenize(self.normalizer.normalize(token))\n",
    "        if mode == 4:\n",
    "            return word_tokenize(token)\n",
    "```\n",
    "\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=4>\n",
    "بنابراین برای جمله بندی از خود هضم استفاده می‌کنیم.\n",
    "جملات را نرمالایز نیز میکنیم که از غلط های ویرایشی بکاهیم.\n",
    "            </font>\n",
    "</div>\n",
    "\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "دقت کنید که برای تعیین کردن \n",
    "span بازه های مشخص شده اسپن نسبت به جملات تعیین میگردد \n",
    "که فرض معقولی می ‌باشد (‌با توجه به داک اگر بخواهیم اسپن کلی را بدست آوریم کافیت اسپن فعلی را با طول جملات قبلی جمع کنیم اما توجه دارید که این کار واقعا نیاز نیست و بخاطر مثال های درون داک تمرین این کار را انجام نمیدهیم و اسپن بازه های خواسته شده را از ابتدای جمله تشخیص داده شده توسط هضم بیان می‌کنیم.)\n",
    "            </font>\n",
    "</div>\n",
    "\n",
    "### لود کردن موارد مورد نیاز\n",
    "\n",
    "```c\n",
    "        self.normalizer = Normalizer()\n",
    "        self.lemmatizer = Lemmatizer()\n",
    "        self.stemmer = Stemmer()\n",
    "        self.tagger = POSTagger(model='./resources/postagger.model')\n",
    "        self.chunker = Chunker(model='resources/chunker.model')\n",
    "        self.vehicles = {s : True  for s in pd.read_csv(\"resources/vehicles_final.csv\")[\"vehicles\"].unique()}\n",
    "        self.places = {s : True  for s in pd.read_csv(\"resources/places.csv\")[\"place\"].unique()}\n",
    "```\n",
    "\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=4>\n",
    "تگر و چانکر و همچنین دیگر موارد هضم را لود میکنیم همچنین برای استفاده رجکس های مورد نیاز برای تشخصی مبدا و مقصد را لود می‌کنیم\n",
    "همچنین یک رجکس از دیتا ست وسایل نقلیه ایجاد می‌کنیم زیرا در قسمت های بعدی نیاز است.\n",
    "            </font>\n",
    "</div>\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=4>\n",
    "فرمت رجکس ها به شرح زیر است <br>\n",
    "به + مقصد\n",
    "<br>\n",
    "(افعال نظیر آمدم رفتم)‌ + مقصد\n",
    "<br>\n",
    "از + مبدا\n",
    "<br>\n",
    "برای پیدا کردن NP آنها ابتدا تمام ‌‌NP ها را با کمک هضم پیدا می‌کنیم سپس اسپن آنهارا بدست‌ می‌اوریم \n",
    "حال که اسپن NP هارا داریم \n",
    "اسپن اولین کلمه پیدا شده برای مقصد یا مبدا را پیدا می‌کنیم.\n",
    "این کار به کمک رجکس های شرح داده شده صورت می‌گیرد.\n",
    "سپس میبینیم که آن اسپن در کدامین اسپن NP ها  می‌گنجند.\n",
    "آن NP را انتخاب می‌کنیم.\n",
    "            </font>\n",
    "</div>\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=4>\n",
    "پیدا کردن وسایل نقلیه <br>\n",
    "از ‌NP های پیدا شده بررسی می‌کنیم که کدام یک طولانی ترین وسیله نقلیه درون دیتاست را داراست سپس آن وسیله نقلیه را بررسی می‌کنیم که آیا قبل از آن با آمده است یا نه \n",
    "زیرا رجکس وسیله نقلیه بصورت پیش فرض زیر است\n",
    "<br>\n",
    "با + وسیله نقلیه \n",
    "<br>\n",
    "اما از آنجا که هدف اصلی پیدا کردن وسیله نقلیه است وسایل نقلیه ای که قبل آنها با نیست را نیز نگه می‌داریم.\n",
    "            </font>\n",
    "</div>\n",
    "\n",
    "# الگوریتم اصلی\n",
    "\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=4>\n",
    "حال به توضیح تابع run می پردازیم.\n",
    "دقت نمایید که به توضیخ خط به خط نمی‌پردازیم و تنها ایده های هر کیس را مطرح می‌کنیم\n",
    "ابتدا توضیح می‌دهیم که چه فرایندی روی جمله صورت می‌پذیرد.\n",
    "            </font>\n",
    "</div>\n",
    "\n",
    "```c\n",
    "sentence = self.get_analysis(sen, 4)  \n",
    "chunked = self.chunker.parse(self.tagger.tag(sentence))\n",
    "tree = tree2brackets(chunked)\n",
    "NP_W = self.find_NP(tree)\n",
    "NP_W_span=self.find_spans(NP_W,sen)\n",
    "posibble_vehicles = self.find_vehicles(NP_W)\n",
    "checked_vehicles,spans,posibble_vehicles = self.check_is_vehicle_after_ba(posibble_vehicles, sen)\n",
    "```\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=4>\n",
    "این کار روی تمام جملات صورت میگیرد در پایان وسایلی که قبل آنها با امده است با اسپن هر یک از NP ها را داریم \n",
    "همچنین ذکر این نکته نیز حاءز اهمیت است که اگر با رجکس با حرف اضافه به مقصدی پیدا نشود سراغ افعال می‌رویم و مقصد را بدان شکل پیدا می‌کنیم.\n",
    "حال به توضیح هر کیس می پردازیم.\n",
    "<ol>\n",
    "  <li>وسیله نقلیه که قبل آن با باشد پیدا شود: در این حالت وسیله نقلیه را قرار داده و مقصد و مبدا را پیدا می‌کنیم. </li>\n",
    "  <li>وسیله نقلیه قبل آن با نباشد در این حالت وسیله نقلیه را مقداری ریلکس تر برخورد کرده و اگر مبدا و مقصدی پیدا شد فرض می‌کنیم همان وسیله بوده است .\n",
    "  دقت کنید ممکن است مبدا و مقصدی پیدا نشود.\n",
    "  </li>\n",
    "  <li>اگر در ‌NP ها وسیله نقلیه پیدا نشود در این صورت دست به دامان رجکس شده و اولین وسیله نقلیه پیدا شده توسط رجکس را به عنوان وسیله نقلیه قرار داده و مبدا و مقصد را در صورت وجود پیدا می‌کنیم.</li>\n",
    "</ol>\n",
    "            </font>\n",
    "</div>\n",
    "\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=4>\n",
    "اگر وسیله نقلیه پیدا نشود دیگر مبدا و مقصدی نیز پیدا نمی‌کنیم.\n",
    "            </font>\n",
    "</div>\n",
    "\n",
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=6>\n",
    "ماژول خواسته شده ماژول در فایل زیر است.\n",
    "            </font>\n",
    "</div>\n",
    "\n",
    "```c\n",
    "Vehicles.py\n",
    "Vehicle_detector\n",
    "\n",
    "from Vehicles import Vehicle_detector\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb34c0",
   "metadata": {},
   "source": [
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=6>\n",
    "چند مثال\n",
    "            </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2bd272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mloading model ... \u001b[0m\n",
      "\u001b[34mloading tagger ... \u001b[0m\n",
      "\u001b[34mloading chuncker ... \u001b[0m\n",
      "\u001b[34mreading dataset ... \u001b[0m\n",
      "\u001b[34mloading regex ... \u001b[0m\n",
      "\u001b[34mloading completed\u001b[0m\n",
      "case 1 : \n",
      "فردا با فولکس از تهران به اصفهان می‌روم\n",
      "[{'from': 'تهران', 'from_span': (17, 22), 'to': 'فولکس', 'to_span': (8, 13), 'vehicle': 'فولکس', 'vehicle_span': (8, 13)}]\n",
      "از جنگل های شمال با تراکتور آمدم شهر.\n",
      "[{'from': 'جنگل های شمال', 'from_span': (3, 16), 'to': 'شهر', 'to_span': (33, 36), 'vehicle': 'تراکتور', 'vehicle_span': (20, 27)}]\n",
      "از کشور بیگانگان با هواپیما با چشمانی پر خون به مام وطن باز گشت.\n",
      "[{'from': 'کشور بیگانگان', 'from_span': (3, 16), 'to': 'مام وطن', 'to_span': (48, 55), 'vehicle': 'هواپیما', 'vehicle_span': (20, 27)}]\n",
      "case 2 : \n",
      "کشتی مسافران را از بندر امام خمینی به بندر دبی برد.\n",
      "[{'from': 'بندر امام خمینی', 'from_span': (19, 34), 'to': 'بندر دبی', 'to_span': (38, 46), 'vehicle': 'کشتی', 'vehicle_span': (0, 4)}]\n",
      "case 3 : \n",
      "نیرو های امدادی توسط هلیکوپترشان مادربزرگم را از حیاط پشتی به بیمارستان بردند.\n",
      "[{'from': 'حیاط پشت', 'from_span': (49, 57), 'to': 'بیمارستان', 'to_span': (62, 71), 'vehicle': 'هلیکوپتر', 'vehicle_span': (21, 29)}]\n",
      "case 4\n",
      "[{'from': '', 'from_span': (-1, -1), 'to': '', 'to_span': (-1, -1), 'vehicle': '', 'vehicle_span': (-1, -1)}]\n"
     ]
    }
   ],
   "source": [
    "from Vehicles import Vehicle_detector\n",
    "vehicle_detector = Vehicle_detector()\n",
    "print(\"case 1 : \")\n",
    "print(\"فردا با فولکس از تهران به اصفهان می‌روم\")\n",
    "print(vehicle_detector.run(\"فردا به فولکس از تهران به اصفهان می‌روم.\"))\n",
    "print(\"از جنگل های شمال با تراکتور آمدم شهر.\")\n",
    "print(vehicle_detector.run(\"از جنگل های شمال با تراکتور آمدم شهر.\"))\n",
    "print(\"از کشور بیگانگان با هواپیما با چشمانی پر خون به مام وطن باز گشت.\")\n",
    "print(vehicle_detector.run(\"از کشور بیگانگان با هواپیما با چشمانی پر خون به مام وطن باز گشت.\"))\n",
    "\n",
    "print(\"case 2 : \")\n",
    "print(\"کشتی مسافران را از بندر امام خمینی به بندر دبی برد.\")\n",
    "print(vehicle_detector.run(\"کشتی مسافران را از بندر امام خمینی به بندر دبی برد.\"))\n",
    "\n",
    "print(\"case 3 : \")\n",
    "print(\"نیرو های امدادی توسط هلیکوپترشان مادربزرگم را از حیاط پشتی به بیمارستان بردند.\")\n",
    "print(vehicle_detector.run(\"نیرو های امدادی توسط هلیکوپترشان مادربزرگم را از حیاط پشتی به بیمارستان بردند.\"))\n",
    "\n",
    "print(\"case 4\")\n",
    "print(vehicle_detector.run(\"از شمال شیراز به جنوب اصفهان رفتم.\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106f576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile Vehicles.py\n",
    "from __future__ import unicode_literals\n",
    "import pandas as pd\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import re\n",
    "from termcolor import colored\n",
    "import pandas as pd\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import re\n",
    "from termcolor import colored\n",
    "class Vehicle_detector:\n",
    "    def __init__(self):\n",
    "        self.mode = True\n",
    "        print(colored(\"loading model ... \",\"blue\"))\n",
    "        self.normalizer = Normalizer()\n",
    "        self.lemmatizer = Lemmatizer()\n",
    "        self.stemmer = Stemmer()\n",
    "        print(colored(\"loading tagger ... \",\"blue\"))\n",
    "        self.tagger = POSTagger(model='./resources/postagger.model')\n",
    "        print(colored(\"loading chuncker ... \",\"blue\"))\n",
    "        self.chunker = Chunker(model='resources/chunker.model')\n",
    "        print(colored(\"reading dataset ... \",\"blue\"))\n",
    "        self.vehicles = {s : True  for s in pd.read_csv(\"resources/vehicles_final.csv\")[\"vehicles\"].unique()}\n",
    "        self.places = {s : True  for s in pd.read_csv(\"resources/places.csv\")[\"place\"].unique()}\n",
    "        f = open(\"resources/dest_verbs.txt\",\"r\",encoding = \"utf-8\")\n",
    "        self.dest_verbs = [l.strip() for l in f.readlines()]\n",
    "        f.close()\n",
    "        print(colored(\"loading regex ... \",\"blue\"))\n",
    "        self.NP_regex = \"\\[[^\\[]* NP\\]\"\n",
    "        self.VP_regex = \"\\[[^\\[]* VP\\]\"\n",
    "        self.After_ba = \"(\\\\S+)?\" + \"( |\\u200c)\"\n",
    "        self.BA = \"با\"\n",
    "        self.SPAN_finder = \"(\\\\S+)?\" + \"(\" + \" \" + \"|\" + \"‌\" + \")\"\n",
    "        self.AZ_source = \"از (\\\\S+)( )?\"\n",
    "        self.BE_dest = \"به (\\\\S+)( )?\"\n",
    "        self.FIND_BY_verb = \"(\"\n",
    "        size = len(self.dest_verbs)-1\n",
    "        for verb in range(size):\n",
    "            v = self.dest_verbs[verb].strip()\n",
    "            self.FIND_BY_verb = self.FIND_BY_verb+ v+\"|\"\n",
    "        self.FIND_BY_verb = self.FIND_BY_verb+self.dest_verbs[size]+\") \"+\"(\\\\S+)\"\n",
    "        self.VEHICLE_REGEX = \"( )?(\"\n",
    "        for key in self.vehicles:\n",
    "            self.VEHICLE_REGEX = self.VEHICLE_REGEX + key+\"|\"\n",
    "        self.VEHICLE_REGEX = self.VEHICLE_REGEX[0:-1]+\")( |شان|تان|مان|اش|ام|ت|م|ش)?\"\n",
    "        print(colored(\"loading completed\",\"blue\"))\n",
    "\n",
    "    def get_analysis(self,token, mode = 0):\n",
    "        assert 0 <= mode < 5 ,\"0  <= mode < 5\"\n",
    "        if mode == 0:\n",
    "            return self.lemmatizer.lemmatize(token)\n",
    "        if mode == 1:\n",
    "            return self.stemmer.stem(token)\n",
    "        if mode == 2:\n",
    "            return self.normalizer.normalize(token)\n",
    "        if mode == 3:\n",
    "            return sent_tokenize(self.normalizer.normalize(token))\n",
    "        if mode == 4:\n",
    "            return word_tokenize(token)\n",
    "    def ste_lem(self,sentence , mode = 0):\n",
    "        assert 0 <= mode <= 1 ,\"mode 1 or 0\"\n",
    "        return [self.get_analysis(s,1-mode) for s in sentence]\n",
    "\n",
    "    def find_NP(self,chunked_tree):\n",
    "        res = re.findall(self.NP_regex, chunked_tree)\n",
    "        return [\" \".join([self.get_analysis(w) for w in word[1:-4].strip().replace(\"\\u200c\", \" \").split()]).strip() for word in res]\n",
    "\n",
    "    def find_VP(self,chunked_tree):\n",
    "        res = re.findall(self.VP_regex, chunked_tree)\n",
    "        return [\" \".join([self.get_analysis(w) for w in word[1:-4].strip().replace(\"\\u200c\", \" \").split()]).strip() for word in res]\n",
    "\n",
    "    def find_vehicles_each(self,word):\n",
    "        if self.vehicles.get(word):\n",
    "            return word\n",
    "        else:\n",
    "            False\n",
    "    def set_mode(self,mode):\n",
    "        self.mode = mode\n",
    "\n",
    "    def take_apart_words(self,sen):\n",
    "        candidate_words = []\n",
    "        for i in sen:\n",
    "            if len(i.split()) >= 1:\n",
    "                strr = \"\"\n",
    "                splitted_words = i.split()\n",
    "                longest_accepted_word = \"\"\n",
    "                for j in splitted_words:\n",
    "                    strr += j\n",
    "                    if self.find_vehicles_each(strr):\n",
    "                        longest_accepted_word = strr\n",
    "                    strr += \" \"\n",
    "                if longest_accepted_word:\n",
    "                    candidate_words.append(longest_accepted_word.strip())\n",
    "        return candidate_words\n",
    "\n",
    "    def check_is_vehicle_after_ba(self,vehicles, sen):\n",
    "        checked_vehicles = []\n",
    "        spans = []\n",
    "        possible = []\n",
    "        for vehicle in vehicles:\n",
    "            vehicle_parts = vehicle.split()\n",
    "            regex = self.BA+\" \"\n",
    "            size = len(vehicle_parts)-1\n",
    "            for index,part in enumerate(vehicle_parts):\n",
    "                regex += (part + self.After_ba) if  index != size else part\n",
    "            regex = regex.strip()\n",
    "            x = re.search(regex, sen)\n",
    "            if x:\n",
    "                start, end = x.span()\n",
    "                checked_vehicles.append(vehicle)\n",
    "                spans.append((start+3,end))\n",
    "            x = re.search(regex[3:], sen)\n",
    "            if x:\n",
    "                start, end = x.span()\n",
    "            possible.append((vehicle,start,end))\n",
    "            \n",
    "        return checked_vehicles,spans,possible \n",
    "\n",
    "    def find_vehicles(self,sen):\n",
    "        words = self.take_apart_words(sen)\n",
    "        if len(words) == 0:\n",
    "            pass\n",
    "        return words\n",
    "\n",
    "    def find_spans(self,NP_Ws, sen: str):\n",
    "        copy_sen = sen\n",
    "        base = 0\n",
    "        spans = []\n",
    "        for NP_W in NP_Ws:\n",
    "            NP_W_parts = NP_W.split(\" \")\n",
    "            regex = \"\"\n",
    "            size = len(NP_W_parts)-1\n",
    "            for index,part in enumerate(NP_W_parts):# \"(\\\\S+)?\" + \"(\" + \" \" + \"|\" + \"‌\" + \")\"\n",
    "                regex += (part + self.SPAN_finder) if index != size else part\n",
    "            \n",
    "            regex = regex.strip()\n",
    "            x = re.search(regex, copy_sen)\n",
    "            start, end = x.span()\n",
    "            spans.append((NP_W, start + base, end + base))\n",
    "            copy_sen=copy_sen[end:]\n",
    "            base = base + end\n",
    "        return(spans)    \n",
    "\n",
    "    def check_places(self,sen, NP_W_span):\n",
    "        sources = []\n",
    "        destinations = []\n",
    "        source = re.search(self.AZ_source, sen)\n",
    "        if source:\n",
    "            posibble_source_start, posibble_source_end = source.span(1)\n",
    "            for NP_W in NP_W_span:\n",
    "                if (NP_W[1] <= posibble_source_start <= posibble_source_end <= NP_W[2] or NP_W[1] == posibble_source_start):\n",
    "                    sources.append(NP_W)\n",
    "                    break\n",
    "\n",
    "        destination = re.search(self.BE_dest, sen)\n",
    "        if destination:\n",
    "            posibble_destination_start, posibble_destination_end = destination.span(1)\n",
    "            for NP_W in NP_W_span:\n",
    "                if (NP_W[1] <= posibble_destination_start <= posibble_destination_end <= NP_W[2] or NP_W[1] == posibble_destination_start):\n",
    "                    destinations.append(NP_W)\n",
    "                    break\n",
    "        if len(destinations) == 0:\n",
    "            destination = re.search(self.FIND_BY_verb, sen)\n",
    "            if destination:\n",
    "                s, e = destination.span(2)\n",
    "                if sen[e-1] == \".\":\n",
    "                    e = e-1\n",
    "                destinations.append((sen[s:e],s,e)) #TODO check by amirmahdi\n",
    "        if len(destinations) == 0:\n",
    "            destinations = [(\"\",-1,-1)]\n",
    "        if len(sources) == 0:\n",
    "            sources = [(\"\",-1,-1)]\n",
    "        return sources[0], destinations[0] \n",
    "    def create_output(self,source = \"\",s_span = (-1,-1),dest = \"\",dest_span = (-1,-1),vehicle = \"\",vehicle_span = (-1,-1)):\n",
    "        return {\n",
    "            \"from\":source,\n",
    "            \"from_span\":s_span,\n",
    "            \"to\":dest,\n",
    "            \"to_span\":dest_span,\n",
    "            \"vehicle\":vehicle,\n",
    "            \"vehicle_span\":vehicle_span\n",
    "        }\n",
    "\n",
    "    def run(self,input:str):\n",
    "        outputs = []\n",
    "        for ind, sen in enumerate(self.get_analysis(input, 3)): \n",
    "            output = {} \n",
    "            try:\n",
    "                sentence = self.get_analysis(sen, 4)  \n",
    "                chunked = self.chunker.parse(self.tagger.tag(sentence))\n",
    "                tree = tree2brackets(chunked)\n",
    "                NP_W = self.find_NP(tree)\n",
    "                NP_W_span=self.find_spans(NP_W,sen)\n",
    "                posibble_vehicles = self.find_vehicles(NP_W)\n",
    "                checked_vehicles,spans,posibble_vehicles = self.check_is_vehicle_after_ba(posibble_vehicles, sen)\n",
    "                if len(checked_vehicles) > 0: #TODO check by amir mahdi \n",
    "                    v = checked_vehicles[0]\n",
    "                    v_s = spans[0]\n",
    "                    source, destination =self.check_places(sen,NP_W_span)\n",
    "                    output = self.create_output(\n",
    "                        source=source[0],\n",
    "                        s_span=(source[1],source[2]),\n",
    "                        dest=destination[0],\n",
    "                        dest_span=(destination[1],destination[2]),\n",
    "                        vehicle=v,\n",
    "                        vehicle_span=v_s\n",
    "                    )\n",
    "                elif len(posibble_vehicles) > 0:\n",
    "                    v = posibble_vehicles[0][0]\n",
    "                    v_s = (posibble_vehicles[0][1],posibble_vehicles[0][2])\n",
    "                    if self.mode:\n",
    "                        source, destination =self.check_places(sen,NP_W_span)\n",
    "                        output = self.create_output(\n",
    "                            source=source[0],\n",
    "                            s_span=(source[1],source[2]),\n",
    "                            dest=destination[0],\n",
    "                            dest_span=(destination[1],destination[2]),\n",
    "                            vehicle=v,\n",
    "                            vehicle_span=v_s\n",
    "                        )\n",
    "                    else:\n",
    "                        output = self.create_output(\n",
    "                            vehicle=v,\n",
    "                            vehicle_span=v_s\n",
    "                        )\n",
    "                else:\n",
    "                    output = self.create_output()\n",
    "                    v = re.search(self.VEHICLE_REGEX, sen) #TODO\n",
    "                    if v:\n",
    "                        s, e = v.span(2)\n",
    "                        if s > 2 and sen[s-3:s-1] == self.BA and e-s > 2:\n",
    "                            source, destination =self.check_places(sen,NP_W_span)\n",
    "                            output = self.create_output(\n",
    "                            source=source[0],\n",
    "                            s_span=(source[1],source[2]),\n",
    "                            dest=destination[0],\n",
    "                            dest_span=(destination[1],destination[2]),\n",
    "                            vehicle=sen[s:e],\n",
    "                            vehicle_span=(s,e)\n",
    "                            )\n",
    "                        elif e-s > 1:\n",
    "                            if self.mode:\n",
    "                                source, destination =self.check_places(sen,NP_W_span)\n",
    "                                output = self.create_output(\n",
    "                                source=source[0],\n",
    "                                s_span=(source[1],source[2]),\n",
    "                                dest=destination[0],\n",
    "                                dest_span=(destination[1],destination[2]),\n",
    "                                vehicle=sen[s:e],\n",
    "                                vehicle_span=(s,e)\n",
    "                                )\n",
    "                            else:\n",
    "                                output = self.create_output(\n",
    "                                vehicle=sen[s:e],\n",
    "                                vehicle_span=(s,e)\n",
    "                                )\n",
    "            except:\n",
    "                output = self.create_output()\n",
    "            outputs.append(output)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3aa624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mloading model ... \u001b[0m\n",
      "\u001b[34mloading tagger ... \u001b[0m\n",
      "\u001b[34mloading chuncker ... \u001b[0m\n",
      "\u001b[34mreading dataset ... \u001b[0m\n",
      "\u001b[34mloading regex ... \u001b[0m\n",
      "\u001b[34mloading completed\u001b[0m\n",
      "--------------------------------\n",
      "از تهران با دویست و شش برادر دوستم به خونه عموی پدرم رفتیم.\n",
      "[{'from': 'تهران', 'from_span': (3, 8), 'to': 'خونه', 'to_span': (38, 42), 'vehicle': 'دویست و شش', 'vehicle_span': (12, 22)}]\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "با پراید دنده اتومات پدرم به خونه برادر عموم رفتیم.\n",
      "[{'from': '', 'from_span': (-1, -1), 'to': 'خونه برادر عموم', 'to_span': (29, 44), 'vehicle': 'پراید', 'vehicle_span': (3, 8)}]\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "پدرم با پراید آمد اصفهان.\n",
      "[{'from': '', 'from_span': (-1, -1), 'to': 'اصفهان', 'to_span': (18, 24), 'vehicle': 'پراید', 'vehicle_span': (8, 13)}]\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "از بندرعباس تا شمال را با دوچرخه رکاب زدیم.\n",
      "[{'from': 'بندرعباس', 'from_span': (3, 11), 'to': '', 'to_span': (-1, -1), 'vehicle': 'دوچرخه', 'vehicle_span': (26, 32)}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from Vehicles import Vehicle_detector\n",
    "vehicle_detector = Vehicle_detector()\n",
    "sample = [line.strip() for line in open(\"input.txt\",\"r\",encoding=\"utf-8\").readlines()]\n",
    "for s in random.sample(sample,4):\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"{s}\")\n",
    "    print(vehicle_detector.run(s))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0498ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Vehicles_with_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Vehicles_with_test.py\n",
    "from Vehicles import Vehicle_detector\n",
    "vehicle_detector = Vehicle_detector()\n",
    "f = open(\"input.txt\",\"r\",encoding= \"utf-8\")\n",
    "final = \"\"\n",
    "for index,line in enumerate(f.readlines()):\n",
    "    if index == 35:\n",
    "        print(index)\n",
    "    else:\n",
    "        print(index)\n",
    "    res = vehicle_detector.run(line.strip())\n",
    "    for a in res:\n",
    "        s = {\n",
    "            \"index\":index,\n",
    "            \"input str\" : line.strip(),\n",
    "            \"result\" : a,\n",
    "            \"detected_vehicle\":a[\"vehicle\"],\n",
    "            \"v_span\":a[\"vehicle_span\"],\n",
    "            \"tested_V_span\":line[a[\"vehicle_span\"][0]:a[\"vehicle_span\"][1]],\n",
    "            \"detected_from\":a[\"from\"],\n",
    "            \"s_span\":a[\"from_span\"],\n",
    "            \"tested_S_span\":line[a[\"from_span\"][0]:a[\"from_span\"][1]],\n",
    "            \"detected_to\":a[\"to\"],\n",
    "            \"d_span\":a[\"to_span\"],\n",
    "            \"tested_D_span\":line[a[\"to_span\"][0]:a[\"to_span\"][1]],\n",
    "        }\n",
    "        final+=\"\\n\".join([f\"{str(key)} : {s[key]}\" for key in s])+\"\\n\"\n",
    "    final+=\"\\n\\n\"\n",
    "f.close()\n",
    "f = open(\"res.txt\",\"w\",encoding=\"utf-8\")\n",
    "f.write(final)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84beb15e057ca0c3811693b28eab1b970102869956d690ca4f596ce5f226a4ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('generalAI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
