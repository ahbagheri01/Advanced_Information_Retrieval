{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git clone https://github.com/facebookresearch/fastText.git\n",
    "\n",
    "cd fastText\n",
    "\n",
    "make\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make sure you have run ./DATA/preprocess.ipynb first \n",
    "### make sure you have cloned and made fasttext\n",
    "```\n",
    "mkdir fasttext (already created just cd)\n",
    "git clone https://github.com/facebookresearch/fastText.git\n",
    "cd fastTex\n",
    "make\n",
    "```\n",
    "### make sure for first time you run model.preprocess(pre = False ,dim=400, epoch=20 , lr = 0.06 , ws = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from typing import List,Tuple\n",
    "import spacy\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial import distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    y = np.exp(x - np.max(x))\n",
    "    return y / y.sum()\n",
    "\n",
    "class Fast_text_TF_IDF_IR:\n",
    "    def __init__(self,t = \"lemma\" , c_soft = True):\n",
    "        self.documents = json.load(open(\"DATA/crawler/data/NLP.json\",\"r\"))\n",
    "        self.t = t\n",
    "        self.mapping = None\n",
    "        self.idf = None\n",
    "        self.train_data_path = None\n",
    "        if t == \"lemma\":  \n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/Module_data/abstract_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/Module_data/idf_abstract.json\",\"r\"))\n",
    "        else:\n",
    "            self.tokenizer = lambda s : [token for token in word_tokenize(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_not_lemma_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/Module_data/abstract_not_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/Module_data/idf_abstract_not_lemma.json\",\"r\"))\n",
    "        self.emmbeding = None\n",
    "        self.mapping = {key : int(self.mapping[key]) for key in self.mapping}\n",
    "        self.idf = {int(key) : float(self.idf[key]) for key in self.idf}\n",
    "        self.c_soft = lambda x : x\n",
    "        if c_soft:\n",
    "            self.c_soft = lambda x : softmax(x)\n",
    "        self.doc_emb = {}\n",
    "        self.dim = 300\n",
    "    def is_c_(self,w):\n",
    "        try:\n",
    "            self.emmbeding[w]\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    \n",
    "    def preprocess(self,pre = False , ws = 5 ,epoch = 20 ,lr = 0.1, dim = 200):\n",
    "        self.dim = dim\n",
    "        if not pre:\n",
    "            os.system(\"rm ./fasttext/word_embedding.*\")\n",
    "            os.system(f\"./fasttext/fastText/fasttext skipgram -dim {dim} -ws {ws} -epoch {epoch} -lr {lr} -input {self.train_data_path} -output ./fasttext/word_embedding\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.bin\")\n",
    "            self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "            for key in self.documents:\n",
    "                article = self.documents[key]\n",
    "                abstract = article[\"abstract\"]\n",
    "                if (type(abstract) == str):\n",
    "                    try:\n",
    "                        word = self.tokenizer(abstract)\n",
    "                        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "                        c = np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1)\n",
    "                        c = self.c_soft(c)\n",
    "                        self.doc_emb[key] = np.matmul(c,matrix).tolist()[0]\n",
    "                    except:\n",
    "                        print(key,c)\n",
    "            open(\"./fasttext/doc_embedding.json\",\"w\").write(json.dumps(self.doc_emb))\n",
    "\n",
    "        self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "        self.doc_emb = json.load(open(\"./fasttext/doc_embedding.json\",\"r\"))\n",
    "        self.doc_emb = {key : np.array(self.doc_emb[key]).reshape(1,self.dim) for key in self.doc_emb}\n",
    "\n",
    "    def process_q(self,q : np.array) -> List[Tuple]:\n",
    "        return sorted([(key,np.abs(distance.cosine(q,self.doc_emb[key]))) for key in self.doc_emb],key = lambda x : x[1])\n",
    "        \n",
    "\n",
    "    def query(self, input_string:str , k : int = 10) -> List:\n",
    "        word = self.tokenizer(input_string.strip().lower())\n",
    "        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "        c = self.c_soft(np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1))\n",
    "        q = np.matmul(c,matrix)[0]\n",
    "        article_id = self.process_q(q)[:k]\n",
    "        articles = [self.documents[id[0]] for id in article_id]\n",
    "        return (article_id,articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Fast_text_TF_IDF_IR(t = \"lemma\")\n",
    "model.preprocess(pre = True ,dim=400, epoch=20 , lr = 0.06 , ws = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('48709b65320bf979d19c1874aebf6bf8bd85de88', 0.09605803161158266)\n",
      "Just What do You Think You're Doing, Dave?' A Checklist for Responsible Data Use in NLP\n",
      "A key part of the NLP ethics movement is responsible use of data, but exactly what that means or how it can be best achieved remain unclear. This position paper discusses the core legal and ethical principles for collection and sharing of textual data, and the tensions between them. We propose a potential checklist for responsible data (re-)use that could both standardise the peer review of conference submissions, as well as enable a more in-depth view of published research across the community. Our proposal aims to contribute to the development of a consistent standard for data (re-)use, embraced across NLP conferences.\n",
      "('e818c127134e287a4e690d5c1ca0126ff82cfa10', 0.16332067866680544)\n",
      "Socially Responsible NLP\n",
      "As language technologies have become increasingly prevalent, there is a growing awareness that decisions we make about our data, methods, and tools are often tied up with their impact on people and societies. This tutorial will provide an overview of real-world applications of language technologies and the potential ethical implications associated with them. We will discuss philosophical foundations of ethical research along with state of the art techniques. Through this tutorial, we intend to provide the NLP researcher with an overview of tools to ensure that the data, algorithms, and models that they build are socially responsible. These tools will include a checklist of common pitfalls that one should avoid (e.g., demographic bias in data collection), as well as methods to adequately mitigate these issues (e.g., adjusting sampling rates or de-biasing through regularization). The tutorial is based on a new course on Ethics and NLP developed at Carnegie Mellon University.\n",
      "('33ec7eb2168e37e3007d1059aa96b9a63254b4da', 0.2902419078616435)\n",
      "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList\n",
      "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.\n",
      "('79b8283f7abb75217698d2d90b536aa68ed3cb14', 0.2902419078616435)\n",
      "Beyond Accuracy: Behavioral Testing of NLP Models with Checklist (Extended Abstract)\n",
      "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.\n",
      "('f70b2f20be241f445a61f33c4b8e76e554760340', 0.5138394520499443)\n",
      "Software Engineering for Machine Learning: A Case Study\n",
      "Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be \"entangled\" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.\n",
      "('d49149072027716458561146fe979f4354129c99', 0.5476501523512608)\n",
      "Using Social Media for Mental Health Surveillance\n",
      "Data on social media contain a wealth of user information. Big data research of social media data may also support standard surveillance approaches and provide decision-makers with usable information. These data can be analyzed using Natural Language Processing (NLP) and Machine Learning (ML) techniques to detect signs of mental disorders that need attention, such as depression and suicide ideation. This article presents the recent trends and tools that are used in this field, the different means for data collection, and the current applications of ML and NLP in the surveillance of public mental health. We highlight the best practices and the challenges. Furthermore, we discuss the current gaps that need to be addressed and resolved.\n",
      "('d8138bcf285547b1b0939c9ad86ecd97a1b82621', 0.5588766697937142)\n",
      "/*icomment: bugs or bad comments?*/\n",
      "Commenting source code has long been a common practice in software development. Compared to source code, comments are more direct, descriptive and easy-to-understand. Comments and sourcecode provide relatively redundant and independent information regarding a program's semantic behavior. As software evolves, they can easily grow out-of-sync, indicating two problems: (1) bugs -the source code does not follow the assumptions and requirements specified by correct program comments; (2) bad comments - comments that are inconsistent with correct code, which can confuse and mislead programmers to introduce bugs in subsequent versions. Unfortunately, as most comments are written in natural language, no solution has been proposed to automatically analyze commentsand detect inconsistencies between comments and source code. This paper takes the first step in automatically analyzing commentswritten in natural language to extract implicit program rulesand use these rules to automatically detect inconsistencies between comments and source code, indicating either bugs or bad comments. Our solution, iComment, combines Natural Language Processing(NLP), Machine Learning, Statistics and Program Analysis techniques to achieve these goals. We evaluate iComment on four large code bases: Linux, Mozilla, Wine and Apache. Our experimental results show that iComment automatically extracts 1832 rules from comments with 90.8-100% accuracy and detects 60 comment-code inconsistencies, 33 newbugs and 27 bad comments, in the latest versions of the four programs. Nineteen of them (12 bugs and 7 bad comments) have already been confirmed by the corresponding developers while the others are currently being analyzed by the developers.\n",
      "('4832c9de76a6ba1c4260ad67c82a79defaac3b30', 0.5609809424636191)\n",
      "Gender as a Variable in Natural-Language Processing: Ethical Considerations\n",
      "Researchers and practitioners in natural-language processing (NLP) and related fields should attend to ethical principles in study design, ascription of categories/variables to study participants, and reporting of findings or results. This paper discusses theoretical and ethical frameworks for using gender as a variable in NLP studies and proposes four guidelines for researchers and practitioners. The principles outlined here should guide practitioners, researchers, and peer reviewers, and they may be applicable to other social categories, such as race, applied to human beings connected to NLP research.\n",
      "('0f266a49416705a0c680b82baafeceae282117a2', 0.5678177799769868)\n",
      "Machine Learning and Natural Language Processing in Mental Health: Systematic Review\n",
      "Background Machine learning systems are part of the field of artificial intelligence that automatically learn models from data to make better decisions. Natural language processing (NLP), by using corpora and learning approaches, provides good performance in statistical tasks, such as text classification or sentiment mining. Objective The primary aim of this systematic review was to summarize and characterize, in methodological and technical terms, studies that used machine learning and NLP techniques for mental health. The secondary aim was to consider the potential use of these methods in mental health clinical practice Methods This systematic review follows the PRISMA (Preferred Reporting Items for Systematic Review and Meta-analysis) guidelines and is registered with PROSPERO (Prospective Register of Systematic Reviews; number CRD42019107376). The search was conducted using 4 medical databases (PubMed, Scopus, ScienceDirect, and PsycINFO) with the following keywords: machine learning, data mining, psychiatry, mental health, and mental disorder. The exclusion criteria were as follows: languages other than English, anonymization process, case studies, conference papers, and reviews. No limitations on publication dates were imposed. Results A total of 327 articles were identified, of which 269 (82.3%) were excluded and 58 (17.7%) were included in the review. The results were organized through a qualitative perspective. Although studies had heterogeneous topics and methods, some themes emerged. Population studies could be grouped into 3 categories: patients included in medical databases, patients who came to the emergency room, and social media users. The main objectives were to extract symptoms, classify severity of illness, compare therapy effectiveness, provide psychopathological clues, and challenge the current nosography. Medical records and social media were the 2 major data sources. With regard to the methods used, preprocessing used the standard methods of NLP and unique identifier extraction dedicated to medical texts. Efficient classifiers were preferred rather than transparent functioning classifiers. Python was the most frequently used platform. Conclusions Machine learning and NLP models have been highly topical issues in medicine in recent years and may be considered a new paradigm in medical research. However, these processes tend to confirm clinical hypotheses rather than developing entirely new information, and only one major category of the population (ie, social media users) is an imprecise cohort. Moreover, some language-specific features can improve the performance of NLP methods, and their extension to other languages should be more closely investigated. However, machine learning and NLP techniques provide useful information from unexplored data (ie, patientsâ€™ daily habits that are usually inaccessible to care providers). Before considering It as an additional tool of mental health care, ethical issues remain and should be discussed in a timely manner. Machine learning and NLP methods may offer multiple perspectives in mental health research but should also be considered as tools to support clinical practice.\n",
      "('8e756b009a9a5feb3896d267909cebd2386deeea', 0.5715165884836293)\n",
      "CNN-Based Automatic Prioritization of Bug Reports\n",
      "Software systems often receive a large number of bug reports. Triagers read through such reports and assign different priorities to different reports so that important and urgent bugs could be fixed on time. However, manual prioritization is tedious and time-consuming. To this end, in this article, we propose a convolutional neural network (CNN) based automatic approach to predict the multiclass priority for bug reports. First, we apply natural language processing (NLP) techniques to preprocess textual information of bug reports and covert the textual information into vectors based on the syntactic and semantic relationship of words within each bug report. Second, we perform the software engineering domain specific emotion analysis on bug reports and compute the emotion value for each of them using a software engineering domain repository. Finally, we train a CNN-based classifier that generates a suggested priority based on its input, i.e., vectored textual information and emotion values. To the best of our knowledge, it is the first CNN-based approach to bug report prioritization. We evaluate the proposed approach on open-source projects. Results of our cross-project evaluation suggest that the proposed approach significantly outperforms the state-of-the-art approaches and improves the average F1-score by more than 24%.\n"
     ]
    }
   ],
   "source": [
    "ranking,articles = model.query(\"performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it\")\n",
    "for a,m in zip(ranking,articles):\n",
    "    print(a)\n",
    "    print(m[\"title\"])\n",
    "    print(m[\"abstract\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84beb15e057ca0c3811693b28eab1b970102869956d690ca4f596ce5f226a4ad"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
