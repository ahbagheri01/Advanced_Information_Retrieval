{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import enum\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from typing import List,Tuple\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make sure you have run ./DATA/pre_process.ipynb first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query_type(enum.Enum):\n",
    "    AUTHOR = \"Author Based\"\n",
    "    TITLE = \"Title Based\"\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "class Boolean_IR:\n",
    "    def __init__(self):\n",
    "        self.author_to_id = json.load(open(\"DATA/Module_data/author_to_id.json\",\"r\"))\n",
    "        self.author_to_doc = json.load(open(\"DATA/Module_data/author_to_doc.json\",\"r\"))\n",
    "        self.authorid_to_num_id = json.load(open(\"DATA/Module_data/authorid_to_num_id.json\",\"r\"))\n",
    "        self.documents = json.load(open(\"DATA/crawler/data/NLP.json\",\"r\"))\n",
    "        self.lemma_title = json.load(open(\"DATA/Module_data/title_lemma.json\",\"r\"))\n",
    "        self.bool_dic_title = json.load(open(\"DATA/Module_data/bool_dic_title.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.title_tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "\n",
    "    def flatten(seflf,l : List[List]) -> List:\n",
    "        return [item for sublist in l for item in sublist]\n",
    "\n",
    "    def word_tokenize_authoe(self,t : str) -> List:\n",
    "        res = word_tokenize(t)\n",
    "        if (res[-1] != \".\"):\n",
    "            return res\n",
    "        res[-2] = res[-2]+res[-1]\n",
    "        return res[:-1]\n",
    "\n",
    "    def pre_process_authors(self) -> None:\n",
    "        self.all_names = list(set(self.flatten([self.word_tokenize_authoe(key) for key in self.author_to_id if not is_int(key)])))\n",
    "        i = iter(range(1,len(self.all_names)+1))\n",
    "        self.w_mapping = defaultdict(lambda : next(i))\n",
    "        self.bool_dic_author = defaultdict(lambda : [])\n",
    "        list(map(lambda x : self.w_mapping[x],self.all_names))\n",
    "        removed_key = []\n",
    "        for key in self.author_to_id:\n",
    "            if not is_int(key) and is_int(self.author_to_id[key]) and key:\n",
    "                i = self.author_to_id[key]\n",
    "                self.bool_dic_author[i] = np.array([self.w_mapping[w] for w in self.word_tokenize_authoe(key)])\n",
    "            else:\n",
    "                removed_key.append(key)\n",
    "        for x in removed_key:\n",
    "            del self.author_to_id[x]\n",
    "    def pre_process_title(self) -> None:\n",
    "        for key in self.bool_dic_title:\n",
    "            self.bool_dic_title[key] = np.array(self.bool_dic_title[key])\n",
    "\n",
    "\n",
    "            \n",
    "    def title_ir(self,wk:str , k : int = 10):\n",
    "        words = np.array([self.lemma_title.get(w,0) for w in wk])\n",
    "        titles = [(key,np.sum([np.sum([item == self.bool_dic_title[key] for item in words ])])) for key in self.documents if type(self.documents[key][\"title\"]) == str]\n",
    "        return sorted(titles , key = lambda x : x[1],reverse=True)[:k]\n",
    "\n",
    "\n",
    "    def author_ir(self,input_wk:str,k) -> List:\n",
    "        names_map = np.array([self.w_mapping.get(w,0) for w in input_wk])\n",
    "        authors = [(key,np.sum([np.sum([name == self.bool_dic_author[self.author_to_id[key]] for name in names_map ])])) for key in self.author_to_id]\n",
    "        return sorted(authors , key = lambda x : x[1],reverse=True)[:k]\n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k : int = 10) -> Tuple[List,List]:\n",
    "        input_string = input_string.lower()\n",
    "        if type == Query_type.TITLE:\n",
    "            mapping = self.title_ir(self.title_tokenizer(input_string.strip().lower()), k)\n",
    "            articles = [self.documents[id[0]] for id in mapping]\n",
    "            return (articles,mapping)\n",
    "        elif type == Query_type.AUTHOR:\n",
    "            names =  self.author_ir(self.word_tokenize_authoe(input_string.strip()),k) \n",
    "            articles = self.flatten([[self.documents[id] for id in self.author_to_doc[self.author_to_id[name[0]]]] for name in names])\n",
    "            return (articles[:k],names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_ir = Boolean_IR()\n",
    "boolean_ir.pre_process_authors()\n",
    "boolean_ir.pre_process_title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mir h. ali', 3)\n",
      "7d4d45f164370c368f00f713dc4b1b2f810b2a01\n",
      "('h. h. muljo', 2)\n",
      "be716f9ea2d22d2855ba06aa466c35ca23023172\n",
      "('h. wallach', 1)\n",
      "d47a682723f710395454687319bb55635e653105\n",
      "('h. cunningham', 1)\n",
      "3e65f572322e192fe36ae52a8a7f025b0685dfc6\n",
      "('ali daud', 1)\n",
      "835ac3cbb41f2ec47718c5491211dd33b64f382b\n",
      "('h. schwarz', 1)\n",
      "2bd2e082913c5366129622fbee1fe24f2dfa696f\n",
      "('h. wang', 1)\n",
      "a1f7045dd66b01f46843592035a26ea407b48982\n",
      "('alexander h. miller', 1)\n",
      "d19b000d782ca90138a38bc7c882a992a99e38c8\n",
      "('h. chase', 1)\n",
      "6e6825c2feada559592e49b093a06fc27214c6c0\n",
      "('patrick h. duffy', 1)\n",
      "5b7929b7e1e74865683e3d1dc5bfd062ef1cab6b\n"
     ]
    }
   ],
   "source": [
    "articles,names = boolean_ir.query(Query_type.AUTHOR,\"mir h. ali\",k = 10)\n",
    "for m,a in zip(articles,names):\n",
    "    print(a)\n",
    "    print(m[\"paperId\"])\n",
    "    # print(m[\"title\"])\n",
    "    # print(m[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('63d8426ba1f51a8525dd19fd8ec92934ec71aea5', 5)\n",
      "A Survey of Data Augmentation Approaches for NLP\n",
      "('013eb12ce5468f79d58bf859653f4929c5a2bd14', 5)\n",
      "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP\n",
      "('69870a6f189a8a68d6dc1e25b6024291711f43a0', 4)\n",
      "Clinical Decision Support Systems: A Survey of NLP-Based Approaches from Unstructured Data\n",
      "('c9b56cb026a38e39bb0228faac57accd6f65e6f7', 3)\n",
      "TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP\n",
      "('982aa0ee48a5fd228fb9fb3b3edd319b8af6f76d', 3)\n",
      "Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs\n",
      "('2a8a2ab581f2e89c9a66e1b353346e1bb86ee6f6', 3)\n",
      "Mixup-Transformer: Dynamic Data Augmentation for NLP Tasks\n",
      "('9dff2ada58cb7a836ec6a23ce8e22d7ce8e0b81f', 3)\n",
      "NLP-Based Approach to Semantic Classification of Heterogeneous Transportation Asset Data Terminology\n",
      "('ddbbf8dd6f4cc4cbf6efba22196e03b37bcd349f', 3)\n",
      "Sentiment Analysis Approaches on Different Data set Domain: Survey\n",
      "('d47a682723f710395454687319bb55635e653105', 2)\n",
      "Language (Technology) is Power: A Critical Survey of “Bias” in NLP\n",
      "('28a5a53dafacebad8a7c47773079caeffb9a5baa', 2)\n",
      "Representing Numbers in NLP: a Survey and a Vision\n"
     ]
    }
   ],
   "source": [
    "articles,mapping = boolean_ir.query(Query_type.TITLE,\"A Survey of Data Augmentation Approaches for NLP\",k = 10)\n",
    "for a,m in zip(mapping,articles):\n",
    "    print(a)\n",
    "    print(m[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['survey', 'datum', 'augmentation', 'approach', 'nlp']\n",
      "[2 4 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "wk = boolean_ir.title_tokenizer(\"A Survey of Data Augmentation Approaches for NLP\".lower())\n",
    "print(wk)\n",
    "words = np.array([boolean_ir.lemma_title.get(w,0) for w in wk])\n",
    "print(words)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84beb15e057ca0c3811693b28eab1b970102869956d690ca4f596ce5f226a4ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('generalAI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
