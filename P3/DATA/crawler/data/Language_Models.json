{"9405cc0d6169988371b2755e573cc28650d14dfe": {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners", "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "38909097", "name": "Alec Radford"}, {"authorId": "49387725", "name": "Jeff Wu"}, {"authorId": "48422824", "name": "Rewon Child"}, {"authorId": "150970919", "name": "D. Luan"}, {"authorId": "2698777", "name": "Dario Amodei"}, {"authorId": "1701686", "name": "Ilya Sutskever"}]}, "6b85b63579a916f705a8e10a49bd8d849d91b1fc": {"paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "title": "Language Models are Few-Shot Learners", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "31035595", "name": "Tom B. Brown"}, {"authorId": "2056658938", "name": "Benjamin Mann"}, {"authorId": "39849748", "name": "Nick Ryder"}, {"authorId": "2065894334", "name": "Melanie Subbiah"}, {"authorId": "152724169", "name": "J. Kaplan"}, {"authorId": "6515819", "name": "Prafulla Dhariwal"}, {"authorId": "2072676", "name": "Arvind Neelakantan"}, {"authorId": "67311962", "name": "Pranav Shyam"}, {"authorId": "144864359", "name": "Girish Sastry"}, {"authorId": "119609682", "name": "Amanda Askell"}, {"authorId": "144517868", "name": "Sandhini Agarwal"}, {"authorId": "1404060687", "name": "Ariel Herbert-Voss"}, {"authorId": "2064404342", "name": "Gretchen Krueger"}, {"authorId": "103143311", "name": "T. Henighan"}, {"authorId": "48422824", "name": "Rewon Child"}, {"authorId": "1992922591", "name": "A. Ramesh"}, {"authorId": "2052152920", "name": "Daniel M. Ziegler"}, {"authorId": "49387725", "name": "Jeff Wu"}, {"authorId": "2059411355", "name": "Clemens Winter"}, {"authorId": "144239765", "name": "Christopher Hesse"}, {"authorId": "2108828435", "name": "Mark Chen"}, {"authorId": "2064673055", "name": "Eric Sigler"}, {"authorId": "1380985420", "name": "Mateusz Litwin"}, {"authorId": "145565184", "name": "Scott Gray"}, {"authorId": "1490681878", "name": "Benjamin Chess"}, {"authorId": "2115193883", "name": "Jack Clark"}, {"authorId": "133740015", "name": "Christopher Berner"}, {"authorId": "52238703", "name": "Sam McCandlish"}, {"authorId": "38909097", "name": "Alec Radford"}, {"authorId": "1701686", "name": "Ilya Sutskever"}, {"authorId": "2698777", "name": "Dario Amodei"}]}, "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6": {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context", "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.", "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "3422912", "name": "Zihang Dai"}, {"authorId": "2109512754", "name": "Zhilin Yang"}, {"authorId": "35729970", "name": "Yiming Yang"}, {"authorId": "143712374", "name": "J. Carbonell"}, {"authorId": "2827616", "name": "Quoc V. Le"}, {"authorId": "145124475", "name": "R. Salakhutdinov"}]}, "85e7d63f75c0916bd350a229e040c5fbb1472e7a": {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners", "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF\u2014better few-shot fine-tuning of language models\u2014a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "4800645", "name": "Tianyu Gao"}, {"authorId": "2064150446", "name": "Adam Fisch"}, {"authorId": "50536468", "name": "Danqi Chen"}]}, "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269": {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code", "abstract": "We introduce Codex, a GPT language model \ufb01ne-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Fur-thermore, we \ufb01nd that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to dif\ufb01cult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including dif\ufb01culty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "2108828435", "name": "Mark Chen"}, {"authorId": "2065005836", "name": "Jerry Tworek"}, {"authorId": "35450887", "name": "Heewoo Jun"}, {"authorId": "153930486", "name": "Qiming Yuan"}, {"authorId": "2117715024", "name": "Henrique Ponde"}, {"authorId": "2053807409", "name": "J. Kaplan"}, {"authorId": "144632352", "name": "Harrison Edwards"}, {"authorId": "51178856", "name": "Yura Burda"}, {"authorId": "2117706920", "name": "Nicholas Joseph"}, {"authorId": "2065151121", "name": "Greg Brockman"}, {"authorId": "2064770039", "name": "Alex Ray"}, {"authorId": "41158993", "name": "Raul Puri"}, {"authorId": "2064404342", "name": "Gretchen Krueger"}, {"authorId": "2136008481", "name": "Michael Petrov"}, {"authorId": "2103414", "name": "Heidy Khlaaf"}, {"authorId": "144864359", "name": "Girish Sastry"}, {"authorId": "2051714782", "name": "Pamela Mishkin"}, {"authorId": "1466431052", "name": "Brooke Chan"}, {"authorId": "145565184", "name": "Scott Gray"}, {"authorId": "39849748", "name": "Nick Ryder"}, {"authorId": "2068123790", "name": "Mikhail Pavlov"}, {"authorId": "146162186", "name": "Alethea Power"}, {"authorId": "40527594", "name": "Lukasz Kaiser"}, {"authorId": "2400764", "name": "Mohammad Bavarian"}, {"authorId": "2059411355", "name": "Clemens Winter"}, {"authorId": "2704719", "name": "Philippe Tillet"}, {"authorId": "9927844", "name": "F. Such"}, {"authorId": "80876468", "name": "D. Cummings"}, {"authorId": "3407285", "name": "Matthias Plappert"}, {"authorId": "2117714459", "name": "Fotios Chantzis"}, {"authorId": "2057742918", "name": "Elizabeth Barnes"}, {"authorId": "1404060687", "name": "Ariel Herbert-Voss"}, {"authorId": "39121861", "name": "William H. Guss"}, {"authorId": "38967461", "name": "Alex Nichol"}, {"authorId": "7309979", "name": "I. Babuschkin"}, {"authorId": "2054519183", "name": "S. Balaji"}, {"authorId": "150298413", "name": "Shantanu Jain"}, {"authorId": "153480842", "name": "A. Carr"}, {"authorId": "2990741", "name": "J. Leike"}, {"authorId": "3381809", "name": "Joshua Achiam"}, {"authorId": "40055795", "name": "Vedant Misra"}, {"authorId": "1404556973", "name": "Evan Morikawa"}, {"authorId": "38909097", "name": "Alec Radford"}, {"authorId": "3555117", "name": "M. Knight"}, {"authorId": "35167962", "name": "Miles Brundage"}, {"authorId": "2117715631", "name": "Mira Murati"}, {"authorId": "2059169400", "name": "Katie Mayer"}, {"authorId": "2930640", "name": "P. Welinder"}, {"authorId": "39593364", "name": "Bob McGrew"}, {"authorId": "2698777", "name": "Dario Amodei"}, {"authorId": "52238703", "name": "Sam McCandlish"}, {"authorId": "1701686", "name": "Ilya Sutskever"}, {"authorId": "2563432", "name": "Wojciech Zaremba"}]}, "6d9727f1f058614cada3fe296eeebd8ec4fc512a": {"paperId": "6d9727f1f058614cada3fe296eeebd8ec4fc512a", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c", "abstract": "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "2471699", "name": "Emily M. Bender"}, {"authorId": "2076288", "name": "Timnit Gebru"}, {"authorId": "1584940075", "name": "Angelina McMillan-Major"}, {"authorId": "2051526200", "name": "Shmargaret Shmitchell"}]}, "babeda48b10a4d638252118f2238d05a06f4ec55": {"paperId": "babeda48b10a4d638252118f2238d05a06f4ec55", "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "abstract": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "50411022", "name": "Moin Nadeem"}, {"authorId": "78850252", "name": "Anna Bethke"}, {"authorId": "145732771", "name": "Siva Reddy"}]}, "a6a83754a0d1e9a8e41c1e9bbdbca32d3b9d1fd3": {"paperId": "a6a83754a0d1e9a8e41c1e9bbdbca32d3b9d1fd3", "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "abstract": "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much \u201cgreener\u201d in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "32246932", "name": "Timo Schick"}, {"authorId": "144418438", "name": "Hinrich Sch\u00fctze"}]}, "e816f788767eec6a8ef0ea9eddd0e902435d4271": {"paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271", "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks", "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of today\u2019s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task\u2019s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "40895369", "name": "Suchin Gururangan"}, {"authorId": "3451494", "name": "Ana Marasovi\u0107"}, {"authorId": "2705113", "name": "Swabha Swayamdipta"}, {"authorId": "46258841", "name": "Kyle Lo"}, {"authorId": "46181066", "name": "Iz Beltagy"}, {"authorId": "145612610", "name": "Doug Downey"}, {"authorId": "144365875", "name": "Noah A. Smith"}]}, "ff0b2681d7b05e16c46dfb71d980cc2f605907cd": {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners", "abstract": "A BSTRACT This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning \u2014\ufb01netuning language models on a collection of datasets described via instructions\u2014substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodi\ufb01ed counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of \ufb01netuning datasets, model scale, and natural language instructions are key to the success of instruction tuning. many tasks", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "144026731", "name": "Jason Wei"}, {"authorId": "40377863", "name": "Maarten Bosma"}, {"authorId": "2664737", "name": "Vincent Zhao"}, {"authorId": "2091768", "name": "Kelvin Guu"}, {"authorId": "40625240", "name": "Adams Wei Yu"}, {"authorId": "144104130", "name": "Brian Lester"}, {"authorId": "2140321952", "name": "Nan Du"}, {"authorId": "2555924", "name": "Andrew M. Dai"}, {"authorId": "2827616", "name": "Quoc V. Le"}]}, "e04a80263d252a3d8a382ba37a249b9345620570": {"paperId": "e04a80263d252a3d8a382ba37a249b9345620570", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "3491117", "name": "Sumanth Dathathri"}, {"authorId": "3064807", "name": "Andrea Madotto"}, {"authorId": "107877389", "name": "Janice Lan"}, {"authorId": "2052239256", "name": "Jane Hung"}, {"authorId": "2059429321", "name": "Eric Frank"}, {"authorId": "34890911", "name": "Piero Molino"}, {"authorId": "2965424", "name": "J. Yosinski"}, {"authorId": "48757909", "name": "Rosanne Liu"}]}, "62d1a3137b01a69443bebf4d92c1990ec512a6a1": {"paperId": "62d1a3137b01a69443bebf4d92c1990ec512a6a1", "title": "Extracting Training Data from Large Language Models", "abstract": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.", "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "authors": [{"authorId": "2483738", "name": "Nicholas Carlini"}, {"authorId": "2444919", "name": "Florian Tram\u00e8r"}, {"authorId": "145217343", "name": "Eric Wallace"}, {"authorId": "40844378", "name": "Matthew Jagielski"}, {"authorId": "1404060687", "name": "Ariel Herbert-Voss"}, {"authorId": "2119194377", "name": "Katherine Lee"}, {"authorId": "145625142", "name": "Adam Roberts"}, {"authorId": "31035595", "name": "Tom B. Brown"}, {"authorId": "143711382", "name": "D. Song"}, {"authorId": "1758110", "name": "\u00da. Erlingsson"}, {"authorId": "3046437", "name": "Alina Oprea"}, {"authorId": "2402716", "name": "Colin Raffel"}]}}