{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "source = \"./crawler/data\"\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_data = json.load(open(f\"{source}/NLP.json\"))\n",
    "all_artcile_data = lang_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper presents a numerical analysis of nonlinear photoconductive (NLP) switching in bulk gallium arsenide. NLP switches are optically activated switches in which the number of electron-hole pairs initially created are multiplied through a nonlinear process. This process is a combination of double injection, trap filling, and avalanche. NLP switches differ from linear photoconductive (LP) switches in which one photon creates one electron-hole pair. The numerical method used to solve the governing NLP switch equations and preliminary results from the computer model are presented.\n"
     ]
    }
   ],
   "source": [
    "article = all_artcile_data[\"bf1bd2dfb4e08868d7e55d1b6fe553761b2299a3\"]\n",
    "# print(article[\"title\"])\n",
    "# print()\n",
    "print(article[\"abstract\"])\n",
    "# print()\n",
    "# print(article[\"authors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1278322"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = iter(range(1,len(all_artcile_data)+1));\n",
    "j = iter(range(1,len(all_artcile_data)+1));\n",
    "doc_to_id = defaultdict(lambda : next(i));\n",
    "author_to_id = defaultdict(lambda :[]);\n",
    "list(map(lambda x : doc_to_id[x],all_artcile_data));\n",
    "def put_author(item):\n",
    "    global author_to_id\n",
    "    for author in item:\n",
    "        name = author[\"name\"].lower()\n",
    "        id = author[\"authorId\"]\n",
    "        author_to_id[name] = id\n",
    "        try:\n",
    "            id = int(id)\n",
    "        except:\n",
    "            id = id\n",
    "        author_to_id[id].append(name)\n",
    "        author_to_id[id] = list(set(author_to_id[id]))\n",
    "\n",
    "[put_author(all_artcile_data[item][\"authors\"]) for item in all_artcile_data]\n",
    "assert max(doc_to_id.values()) == len(all_artcile_data) , \"size does not match\"\n",
    "open(\"Module_data/doc_to_integerID.json\",\"w\").write(json.dumps(doc_to_id))\n",
    "open(\"Module_data/author_to_id.json\",\"w\").write(json.dumps(author_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412495"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_to_doc = defaultdict(lambda :[])\n",
    "for item in all_artcile_data:\n",
    "    ar = all_artcile_data[item]\n",
    "    for author in ar[\"authors\"]:\n",
    "        author_to_doc[author[\"authorId\"]].append(item)\n",
    "open(\"Module_data/author_to_doc.json\",\"w\").write(json.dumps(author_to_doc))  \n",
    "i = iter(range(1,len(author_to_id)+1))\n",
    "authorid_to_num_id = defaultdict(lambda : next(i))\n",
    "list(map(lambda x : authorid_to_num_id[x],[key for key in author_to_id if type(key) == int]))\n",
    "open(\"Module_data/authorid_to_num_id.json\",\"w\").write(json.dumps(authorid_to_num_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "authorid_to_num_id = defaultdict(lambda : next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "flatten = lambda x : [item for subitem in x for item in subitem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "646336"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "i = 0\n",
    "def nx():\n",
    "    global i\n",
    "    i += 1\n",
    "    return i\n",
    "j = 1\n",
    "def nx2():\n",
    "    global j\n",
    "    j += 1\n",
    "    return j\n",
    "title_lemma = defaultdict(lambda : nx())\n",
    "abstract_lemma = defaultdict(lambda : nx2())\n",
    "for key in all_artcile_data:\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"title\"]) == str:\n",
    "        title = article[\"title\"].strip().lower()\n",
    "        for w in nlp(title):\n",
    "            title_lemma[w.lemma_]\n",
    "    if type(article[\"abstract\"]) == str:\n",
    "        abstract = article[\"abstract\"].strip().lower()\n",
    "        for w in nlp(abstract):\n",
    "            abstract_lemma[w.lemma_]\n",
    "open(\"Module_data/title_lemma.json\",\"w\").write(json.dumps(title_lemma))\n",
    "open(\"Module_data/abstract_lemma.json\",\"w\").write(json.dumps(abstract_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895446"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_dic_title = defaultdict(lambda : [])\n",
    "import numpy as np\n",
    "removed_key = []\n",
    "for key in all_artcile_data:\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"title\"]) == str:\n",
    "        title = article[\"title\"].strip().lower()\n",
    "        bool_dic_title[key] = [title_lemma[w.lemma_] for w in nlp(title)]\n",
    "open(\"Module_data/bool_dic_title.json\",\"w\").write(json.dumps(bool_dic_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_data = json.load(open(\"Module_data/title_lemma.json\",\"r\"))\n",
    "abs_data = json.load(open(\"Module_data/abstract_lemma.json\",\"r\"))\n",
    "import numpy as np\n",
    "freq_matrix_title = np.zeros((len(all_artcile_data)+2,len(title_data)+2))\n",
    "freq_matrix_abstract = np.zeros((len(all_artcile_data)+2,len(abs_data)+2))\n",
    "n = 0\n",
    "for key in all_artcile_data:\n",
    "    n += 1\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"title\"]) == str:\n",
    "        title = article[\"title\"].strip().lower()\n",
    "        for w in nlp(title):\n",
    "            if w.lemma_ not in nlp.Defaults.stop_words:\n",
    "                freq_matrix_title[n][title_data[w.lemma_]] += 1\n",
    "    if type(article[\"abstract\"]) == str:\n",
    "        abstract = article[\"abstract\"].strip().lower()\n",
    "        for w in nlp(abstract):\n",
    "            if w.lemma_ not in nlp.Defaults.stop_words:\n",
    "                freq_matrix_abstract[n][abs_data[w.lemma_]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"Module_data/matrix.npz\", freq_matrix_title=freq_matrix_title,freq_matrix_abstract=freq_matrix_abstract)\n",
    "v=np.load(\"Module_data/matrix.npz\")\n",
    "freq_matrix_title = v[\"freq_matrix_title\"]\n",
    "freq_matrix_abstract = v[\"freq_matrix_abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8942, 9188)\n",
      "(8942, 34337)\n"
     ]
    }
   ],
   "source": [
    "print(freq_matrix_title.shape)\n",
    "print(freq_matrix_abstract.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948958"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = len(all_artcile_data)\n",
    "sum_matrix = (freq_matrix_title > 0).sum(axis = 0)\n",
    "idf_title = {int(title_data[key]) : (np.log2(M/sum_matrix[title_data[key]]) if sum_matrix[title_data[key]] != 0 else 0) for key in title_data}\n",
    "M = len(all_artcile_data)\n",
    "sum_matrix = (freq_matrix_abstract > 0).sum(axis = 0)\n",
    "idf_abstract = {int(abs_data[key]) : (np.log2(M/sum_matrix[abs_data[key]]) if sum_matrix[abs_data[key]] != 0 else 0) for key in abs_data}\n",
    "open(\"Module_data/idf_title.json\",\"w\").write(json.dumps(idf_title))\n",
    "open(\"Module_data/idf_abstract.json\",\"w\").write(json.dumps(idf_abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8940"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_ids = [key for key in all_artcile_data]\n",
    "len(article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60152/3948576010.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  tf_title = np.log2(1+freq_matrix_title/freq_matrix_title.max(axis=1).reshape(-1,1))\n",
      "/tmp/ipykernel_60152/3948576010.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  tf_abs = np.log2(1+freq_matrix_abstract/freq_matrix_abstract.max(axis=1).reshape(-1,1))\n"
     ]
    }
   ],
   "source": [
    "tf_title = np.log2(1+freq_matrix_title/freq_matrix_title.max(axis=1).reshape(-1,1))\n",
    "tf_abs = np.log2(1+freq_matrix_abstract/freq_matrix_abstract.max(axis=1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "title_tf = {}\n",
    "asb_tf = {}\n",
    "for key in all_artcile_data:\n",
    "    n += 1\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"title\"]) == str:\n",
    "        title = article[\"title\"].strip().lower()\n",
    "        title_tf[key] = {title_data[w.lemma_] : tf_title[n][title_data[w.lemma_]] for w in nlp(title) if w.lemma_ not in nlp.Defaults.stop_words}\n",
    "    if type(article[\"abstract\"]) == str:\n",
    "        abstract = article[\"abstract\"].strip().lower()\n",
    "        asb_tf[key] = {abs_data[w.lemma_] : tf_abs[n][abs_data[w.lemma_]] for w in nlp(abstract) if w.lemma_ not in nlp.Defaults.stop_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16124667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"Module_data/title_tf.json\",\"w\").write(json.dumps(title_tf))\n",
    "open(\"Module_data/asb_tf.json\",\"w\").write(json.dumps(asb_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8150004"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "flatten = lambda x : [item for subitem in x for item in subitem]\n",
    "not_lemma = \" \".join([all_artcile_data[key][\"abstract\"].lower().strip() for key in all_artcile_data if (type(all_artcile_data[key][\"abstract\"]) == str)])\n",
    "s = \" \".join(flatten([[w.lemma_ for w in nlp(all_artcile_data[key][\"abstract\"].lower().strip())] for key in all_artcile_data if type(all_artcile_data[key][\"abstract\"]) == str]))\n",
    "open(\"../fasttext/fasttext_not_lemma_data.txt\",\"w\").write(not_lemma)\n",
    "open(\"../fasttext/fasttext_data.txt\",\"w\").write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929036"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "def nx():\n",
    "    global i\n",
    "    i += 1\n",
    "    return i\n",
    "from nltk.tokenize import word_tokenize\n",
    "abstract_not_lemma = defaultdict(lambda : nx())\n",
    "for key in all_artcile_data:\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"abstract\"]) == str:\n",
    "        abstract = article[\"abstract\"].strip().lower()\n",
    "        for w in word_tokenize(abstract):\n",
    "            abstract_not_lemma[w]\n",
    "open(\"Module_data/abstract_not_lemma.json\",\"w\").write(json.dumps(abstract_not_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1304348"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_data = json.load(open(\"Module_data/abstract_not_lemma.json\",\"r\"))\n",
    "import numpy as np\n",
    "freq_matrix_abstract = np.zeros((len(all_artcile_data)+2,len(abs_data)+2))\n",
    "n = 0\n",
    "for key in all_artcile_data:\n",
    "    n += 1\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"abstract\"]) == str:\n",
    "        abstract = article[\"abstract\"].strip().lower()\n",
    "        for w in word_tokenize(abstract):\n",
    "            freq_matrix_abstract[n][abs_data[w]] += 1\n",
    "\n",
    "M = len(all_artcile_data)\n",
    "sum_matrix = (freq_matrix_abstract > 0).sum(axis = 0)\n",
    "idf_abstract = {int(abs_data[key]) : (np.log2(M/sum_matrix[abs_data[key]]) if sum_matrix[abs_data[key]] != 0 else 0) for key in abs_data}\n",
    "open(\"Module_data/idf_abstract_not_lemma.json\",\"w\").write(json.dumps(idf_abstract))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84beb15e057ca0c3811693b28eab1b970102869956d690ca4f596ce5f226a4ad"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
