{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "import scipy\n",
    "#765c79514570591907b84b82ebc2766e2c0a0f57\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amirhossein Bagheri 98105621\n",
    "## Mohammad Sadegh majidi Yazdi  98106004\n",
    "## Amirmahdi kousheshi 98171053\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging data\n",
    "\n",
    "<b> skip 3 cell below if you want to use numpy object we already provided and ran this part and saved the output in Module_data you don't have to go throgh whole process it will takes you to downloads 3 Gb data and then needs 15 Gb ram to clean it and create final data.</b>\n",
    "you have to download 4 topics data from [here](https://drive.google.com/drive/folders/19BL85jIEmxre3h8uj6BrFXAn4QoJz6zJ?usp=sharing) and put them in Data folder  it's going to look like this\n",
    "![4 topics](./Module_data/pre_data.jpg)\n",
    "\n",
    "\n",
    "after that run cell below\n",
    "it will need 15 gb ram and will take about 5 minutes to run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DATA/Language_Models.json\n",
      "./DATA/transformers.json\n",
      "./DATA/NLP.json\n",
      "./DATA/natural_language_processing_transformers.json\n",
      "29920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge data\n",
    "merged_data = {}\n",
    "reduced_data = {}\n",
    "for f in os.listdir(\"./DATA/\"):\n",
    "    f = os.path.join(\"./DATA\",f)\n",
    "    if f.endswith(\".json\"):\n",
    "        print(f)\n",
    "        data = json.load(open(f,\"r\"))\n",
    "        for key in data:\n",
    "            merged_data[key] = data[key]\n",
    "            reduced_data[key] = {\"referenceCount\":data[key][\"referenceCount\"],\"citationCount\":data[key][\"citationCount\"],\n",
    "            \"authors\":[l[\"authorId\"] for l in data[key][\"authors\"]],\n",
    "            \"references\":[{\"pid\" : l[\"paperId\"], \"authors\":[k[\"authorId\"] for k in l[\"authors\"]]} for l in data[key][\"references\"]]}\n",
    "        del data\n",
    "        gc.collect()\n",
    "json.dump(merged_data,open(\"./Module_data/data.json\",\"w\"))\n",
    "json.dump(reduced_data,open(\"./Module_data/clean_data.json\",\"w\"))\n",
    "print(len(merged_data))\n",
    "del merged_data\n",
    "del reduced_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors = 11449\n",
      "articles = 6899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(\"./Module_data/clean_data.json\",\"r\"))\n",
    "i = 0\n",
    "def count():\n",
    "    global i\n",
    "    j = i\n",
    "    i += 1\n",
    "    return j\n",
    "x = 0\n",
    "def count2():\n",
    "    global x\n",
    "    j = x\n",
    "    x += 1\n",
    "    return j\n",
    "article_mapping = defaultdict(lambda : count())\n",
    "author_mapping = defaultdict(lambda : count2())\n",
    "for index_ar,key in enumerate(data):\n",
    "    if key == None:\n",
    "        continue\n",
    "    article = data[key]\n",
    "    article_mapping[key]\n",
    "    for index,k in enumerate(article[\"authors\"]):\n",
    "        if k == None:\n",
    "            continue\n",
    "        author_mapping[k]\n",
    "        if index > 5:\n",
    "            break\n",
    "    for index,k in enumerate(article[\"references\"]):\n",
    "        if k[\"pid\"] == None:\n",
    "            continue\n",
    "        article_mapping[k[\"pid\"]]\n",
    "        for index2,m in enumerate(k[\"authors\"]):\n",
    "            if m == None:\n",
    "                continue\n",
    "            author_mapping[m]\n",
    "            if index2 > 5:\n",
    "                break\n",
    "        if index > 9:\n",
    "            break\n",
    "    if index_ar > 1000:\n",
    "        break\n",
    "\n",
    "json.dump(article_mapping,open(\"./Module_data/article_mapping.json\",\"w\"))\n",
    "json.dump(author_mapping,open(\"./Module_data/author_mapping.json\",\"w\"))\n",
    "print(f\"authors = {len(author_mapping)}\")\n",
    "print(f\"articles = {len(article_mapping)}\")\n",
    "del data\n",
    "del author_mapping\n",
    "del article_mapping\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create spare matrix \n",
    "here we create sparse matrix which is graph we need and store it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of authors = 11449\n",
      "total number of edge in authors graph = 87202.0\n",
      "total number of articles = 6899\n",
      "total number of edge in authors graph = 9030.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(\"./Module_data/clean_data.json\",\"r\"))\n",
    "author_mapping = json.load(open(\"./Module_data/author_mapping.json\",\"r\"))\n",
    "article_mapping = json.load(open(\"./Module_data/article_mapping.json\",\"r\"))\n",
    "authors = np.zeros((len(author_mapping),len(author_mapping)))\n",
    "articles = np.zeros((len(article_mapping),len(article_mapping)))\n",
    "for index_ar,key in enumerate(data):\n",
    "    if key == None:\n",
    "        continue\n",
    "    ar_id = article_mapping.get(key,None)\n",
    "    if ar_id == None:\n",
    "        continue\n",
    "    article = data[key]\n",
    "    authors_id_all = []\n",
    "    for index,k in enumerate(article[\"authors\"]):\n",
    "        if k == None:\n",
    "            continue\n",
    "        aut_id = author_mapping.get(k,None)\n",
    "        if aut_id == None:\n",
    "            continue\n",
    "        authors_id_all.append(aut_id)\n",
    "        if index > 5:\n",
    "            break\n",
    "    for index,k in enumerate(article[\"references\"]):\n",
    "        if k[\"pid\"] == None:\n",
    "            continue\n",
    "        ref_id = article_mapping.get(k[\"pid\"],None)\n",
    "        if ref_id == None:\n",
    "            continue\n",
    "        articles[ar_id][ref_id] += 1\n",
    "        for index2,m in enumerate(k[\"authors\"]):\n",
    "            if m == None:\n",
    "                continue\n",
    "            ref_auth_if = author_mapping.get(m,None)\n",
    "            if ref_auth_if == None:\n",
    "                continue\n",
    "            for auth_index in authors_id_all:\n",
    "                authors[auth_index][ref_auth_if] += 1\n",
    "            if index2 > 5:\n",
    "                break\n",
    "        if index > 9:\n",
    "            break\n",
    "    if index_ar > 1000:\n",
    "        break\n",
    "authors = csr_matrix(authors)\n",
    "articles = csr_matrix(articles)\n",
    "print(f\"total number of authors = {len(author_mapping)}\")\n",
    "print(f\"total number of edge in authors graph = {authors.sum()}\")\n",
    "\n",
    "print(f\"total number of articles = {len(article_mapping)}\")\n",
    "print(f\"total number of edge in authors graph = {articles.sum()}\")\n",
    "\n",
    "sparse.save_npz(\"./Module_data/authors_sparse.npz\", authors)\n",
    "sparse.save_npz(\"./Module_data/articles_sparse.npz\", articles)\n",
    "# your_matrix_back = sparse.load_npz(\"yourmatrix.npz\")\n",
    "del data\n",
    "del author_mapping\n",
    "del article_mapping\n",
    "del articles,authors\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84beb15e057ca0c3811693b28eab1b970102869956d690ca4f596ce5f226a4ad"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
